Библиотеки Python для Data Science: продолжение
Урок 4. Оценка и интерпретация полученной модели. Обсуждение курсового проекта.
**********************************Задача************************************
Расскажите, как работает регуляризация в решающих деревьях, какие параметры мы штрафуем в данных алгоритмах?
****************************************************************************
Регуляризация в решающих деревьях используется для контроля переобучения модели. Она заключается в добавлении штрафа за сложность модели в функционал ошибки. В решающих деревьях для регуляризации используются два основных параметра: глубина дерева (max_depth) и минимальное число объектов в листе (min_samples_leaf).

Глубина дерева (max_depth) ограничивает максимальное число узлов в дереве, то есть максимальное число ветвлений, которое может произойти при разбиении данных. Если задать максимальную глубину, то дерево перестанет разбивать данные, когда достигнет этой глубины. Это помогает избежать переобучения.

Минимальное число объектов в листе (min_samples_leaf) ограничивает минимальное число объектов, которые должны находиться в каждом листе дерева. Если задать это значение, то дерево не будет разбивать данные, если в листе останется меньше объектов, чем это значение. Это также помогает избежать переобучения.

Штраф за сложность модели может быть добавлен в функционал ошибки с помощью параметра регуляризации alpha. Этот параметр контролирует величину штрафа за сложность модели. Чем больше значение alpha, тем больше штраф за сложность модели.



**********************************Задача************************************
По какому принципу рассчитывается "важность признака (feature_importance)" в ансамблях деревьев?
****************************************************************************
В ансамблях деревьев, таких как случайный лес или градиентный бустинг, важность признака (feature importance) рассчитывается на основе того, как часто признак используется для разделения данных в деревьях.

В случайном лесе, важность признака определяется путем усреднения значений прироста качества (impurity decrease) для каждого признака по всем деревьям в ансамбле. Прирост качества - это мера того, насколько уменьшается неопределенность (например, энтропия или джини-индекс) при разделении данных на две группы по данному признаку. Чем больше прирост качества, тем более важным считается признак.

В градиентном бустинге, важность признака рассчитывается на основе того, как часто признак используется для разделения данных во всех деревьях, построенных в ходе итераций алгоритма. Важность признака определяется путем усреднения величины градиента функции потерь по данному признаку во всех деревьях. Чем больше величина градиента, тем более важным считается признак.
